
notes:
X is for features
y is for target

Gather Data
	Beautiful soup - web scraper to hop from website to website and get raw data
		from bs4 import BeautifulSoup
	IMDB API - # using the IMDB api available https://github.com/alberanid/imdbpy
		from imdb import IMDb  # to access imdb API
	Selenium - web scraping tool. a robotic process automation tool that can open a website and get information from the website.
		from selenium import webdriver
		
		
		
		
Exploratory Data Analysis



Mangling Data
	StandardScaler 	- scale the features ( X )
		from sklearn.preprocessing import StandardScaler
Imbalance Data
	
	
	Oversample
		from imblearn.over_sampling import SMOTE
	Undersample
		from imblearn.under_sampling import NearMiss		
	Both - SMOTE-TOMEK
		
Feature Selection
	What is RFE ???
		from sklearn.feature_selection import RFE
	Ridge
		from sklearn.linear_model import RidgeCV, Ridge
	Lasso
		from sklearn.linear_model import  LassoCV, Lasso
	Elastinet
		from sklearn.linear_model import  ElasticNetCV, ElasticNet		
		

Feature Selection - fit - remove 0.05 statsmodel Probability % - fit again - check beta and remove fields - start from top!!!!
	scikit learn's Recursive Feature Elimination (RFE) helps one eliminate features if there are too many.	
		
Categorial Data
	One-hot

	
	PCA - where does PCS fit???
		from sklearn.decomposition import PCA

Training
	train_test_split - defaults 75 / 25 split. 
		from sklearn.model_selection import train_test_split
	k fold
		from sklearn.model_selection import KFold
		from sklearn.model_selection import StratifiedKFold
		
	
Models
	RandomForest *
		from sklearn.ensemble import RandomForestClassifier
	Linear Regression
	K Nearest Neighbor *
		from sklearn.neighbors import KNeighborsClassifier
		from sklearn.neighbors import KNeighborsRegressor		
	tree
	Logistic Regression *
		from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
	
			
	clustering 
	Random Forest
		from sklearn.ensemble import RandomForestClassifier
	what is grid swarch?
		from sklearn.model_selection import GridSearchCV
		

		from sklearn.ensemble import ExtraTreesClassifier

	Support Vector Machines
		from sklearn.svm import SVC - classification
		from sklearn.svm import SVR - regression
	SVM (SGD classfier)
		
	
	Decision Tree *
		from sklearn.tree import DecisionTreeClassifier
	XGBoost
		from xgboost import XGBRegressor
		
		
		from sklearn.ensemble import RandomForestRegressor
		from sklearn.ensemble import GradientBoostingRegressor
		from sklearn.linear_model import SGDRegressor
		from sklearn.grid_search import GridSearchCV

	Naive Bayes		
		from sklearn.naive_bayes import BernoulliNB, GaussianNB
		from sklearn.naive_bayes import MultinomialNB
	Linear SVC
	
	
	Arima, Sarimax, MLPN and LSTM
	
	Netural Network
		from sklearn.neural_network import MLPClassifier

unsupervised.
	DBS Scan
	Heirarchical Clustering
		
Metrics

	classification_report
		from sklearn.metrics import classification_report
			accuracy_score
				from sklearn.metrics import accuracy_score
				tp + tn / tp + tn + fp + fn
			precision_score
				from sklearn.metrics import precision_score
				tp = tp + fp
			recall_score
				from sklearn.metrics import recall_score
				ability to fish out the minority class
				tp = tp + fn
		f1_score
			from sklearn.metrics import f1_score
	
	classification_report_imbalanced
		from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report


	cross_val_score
		from sklearn.model_selection import cross_val_score
	Confusion Matrix
		from sklearn.metrics import confusion_matrix
	
	roc_auc_score

	MSE	
		from sklearn.metrics import mean_squared_error
	MAE
		from sklearn.metrics import mean_absolute_error

	Score - sklearn-based score metric

	Neural Network
		Keras - wrapper around Tensorflow
		Tensorflow - Google technology for NN
		CNTK
		PyTorch
	
		
Visualization
	wordcloud - create word cloud in jupyter notebooks
		from wordcloud import WordCloud, STOPWORDS
	historgram
		plt.hist()
	box plot
		sns.boxplot()
		sns.countplot()
		
WYSIWYG Platforms for ML
		Orange
		Knime
		alteryx
		Rapid Miner
		DataRobot





In Amazon ML, you can tune four parameters that affect your model's predictive performance: model size, number of passes, type of shuffling, and regularization. 

The Amazon ML learning algorithm can drop features that don't contribute much to the learning process. To indicate that you want to drop those features, choose the L1 regularization parameter when you create the ML model.
Amazon Machine Learning currently uses an industry-standard logistic regression algorithm to generate models.

what is "has the almost constant recall and f1 score on each C"

----

CRISP-DM - CRISP-DM stands for cross-industry process for data mining. The CRISP-DM methodology provides a structured approach to planning a data mining project. 
Principal component analysis
standford youtube for machine learning - https://www.youtube.com/watch?v=HZ4cvaztQEs&index=3&list=PLA89DCFA6ADACE599
sentdex youtube channel - https://www.youtube.com/watch?v=2BrpKpWwT2A&list=PLQVvvaa0QuDcOdF96TBtRtuQksErCEBYZ

imbalance-learn library =  . smote + Tomek . this process lessen the imbalance of the data. youtube smote - syntehtic minotiryt oversamplete technique

-----

assignment: do movies lab...
Read more about pandas dataframe. fillna's inplace

-----
Notes:
RR of 2 requires win rate of 50% for the strategy to be profitable. The ML will learn the strategy and needs to only be 51% correct for the strategy have positive expectancy.
columns: weeks	months	hours	days	stoch m15	stoch h1	stoch h4	20ma m15	20ma h1		20ma h4		'want'
want = true (now) when (close(16 candles later) - open(now)) => 2*open(now) - SL(now). SL = nearest fractal swing low now.



Problem Statement: Use Machine Learning to learn the WanToo strategy hidden in the data set.
Assumptions: the trading strategy "looks" at 20ma and stochastics data when making it's decision. all these confounders are in the fields / columns.
goals / success metrics: false positive must be less than 51%.
	definitions
		correct :  true positive and true negatives
		errors  : false positive and false negatives
		
		false positive / (true positive + false positive) <50%
		
Risk and limitation: missing confounders on the column.
-----



functions names - small letters 

Class names - capitalized first letter

"None" is a special key word in python.

possible problems with the data:
	null fields / holes
	duplicate entries
	format errors ( date and times . etc )
	standard deviations, mode, mean, etc.
	
Prepare the Data / Data Cleaning:
	fill the null with the median of that column ( feature )
	get rid of that row - throw away data.
	duplication - keep one line only.
	format error - correct the format.

Fields - columns of the raw data
always get data with high granularity.

variables:
	continuous
		float
	discrete
		integer
		categorical
			integer
			string
			
	Terrabyte-size data needs Hadoop or Spark due to size	

Modeling: 
	training phase --> prediction --> target col -> training accuracy A
	testing phase --> prediction --> target col -> training accuracy B
	
	training accuracy A and training accuracy B MUST be similar.
	training accuracy A and training accuracy B is NOT be similar = overfitting
	
Deployment;
	API
	Website
		Flask
		Django
		
Are there any distinct groups in our data? - identify cluster, identify outliers.
outliers are good because they ask more questions. in fraud detection, outliers are the fraudulent transactions.


Multi - objective optimization 
multinomial problem - target is 2 columns / field.
feature selection vs dimensionality reduction = 
	feature selection = given 100 columns, decide which one to keep and which one to delete.
	dimensionality reduction = take 100 columns and create 10 features (thru PCA). then use this 10 features to predict your model.

Tuples: An ordered sequence with a fixed number of elements; e.g., in x = (1, 2, 3), the parentheses makes it a tuple. x = ("Kirk", "Picard", "Spock") — once you've defined this, you can't change it.
Lists: An ordered sequence without a fixed number of elements, e.g., x = [1, 2, 3]. Note the square brackets. x = ["Lord", "of", "the", "Rings"] — this can be changed as you like.
Dictionaries: An unordered collection of key-value pairs, e.g., x = {'Mark': 'Twain', 'Apples': 5}. To retrieve each value (the part after each colon), use its key (the part before each colon). For example, x['Apples'] retrieves the value 5.
Set = {}